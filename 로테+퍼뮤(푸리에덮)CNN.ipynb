{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "로테+퍼뮤(푸리에덮)CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macho-yoo/KW_AI_Hackathon/blob/main/%EB%A1%9C%ED%85%8C%2B%ED%8D%BC%EB%AE%A4(%ED%91%B8%EB%A6%AC%EC%97%90%EB%8D%AE)CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8HzxlQ8JOsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75466c90-6562-4b3c-96ea-2abc5bdf3f62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie1WfnOMOsp1"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/월간 11 운동')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqZobjceOs3s"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from scipy import fftpack\n",
        "from numpy.fft import *\n",
        "# 무시\n",
        "pd.set_option('mode.chained_assignment',  None) # <==== 경고를 끈다\n",
        "\n",
        "train = pd.read_csv('train_features.csv')\n",
        "\n",
        "\n",
        "train_label = pd.read_csv('train_labels.csv')\n",
        "train_y = train_label.label\n",
        "\n",
        "test = pd.read_csv('test_features.csv')\n",
        "submission=pd.read_csv('sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ2u-CCONs2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70db6dc9-b978-4fd5-8aad-b74924352d68"
      },
      "source": [
        "import scipy\n",
        "!pip install transforms3d\n",
        "from transforms3d.axangles import axangle2mat\n",
        "\n",
        "def rolling(data):\n",
        "    for j in np.random.choice(data.shape[0], int(data.shape[0]*2/3)):\n",
        "        data[j] = np.roll(data[j], np.random.choice(data.shape[1]), axis= 0)\n",
        "    return data\n",
        "\n",
        "def rotation(data):\n",
        "    axis = np.random.uniform(low=-1, high=1, size=data.shape[1])\n",
        "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
        "\n",
        "    return np.matmul(data , axangle2mat(axis,angle))\n",
        "\n",
        "def permutation(data, nPerm=4, mSL=10):\n",
        "    data_new = np.zeros(data.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile == True:\n",
        "        segs = np.zeros(nPerm+1, dtype=int)\n",
        "        segs[1:-1] = np.sort(np.random.randint(mSL, data.shape[0]-mSL, nPerm-1))\n",
        "        segs[-1] = data.shape[0]\n",
        "        if np.min(segs[1:]-segs[0:-1]) > mSL:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        data_temp = data[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
        "        data_new[pp:pp+len(data_temp),:] = data_temp\n",
        "        pp += len(data_temp)\n",
        "    return(data_new)\n",
        "\n",
        "\n",
        "def Jitter(data, sigma=0.05):\n",
        "    myNoise = np.random.normal(loc=0, scale=sigma, size=data.shape)\n",
        "    return data+myNoise\n",
        "\n",
        "\n",
        "def combine_aug(data, k, aug_P = 0):\n",
        "    data_ = data.copy()\n",
        "    if aug_P == 0:\n",
        "        if (k+1) % 3 == 0:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = rotation(np.array(data_[600*i:600*(i+1)]))\n",
        "        if (k+1) % 3 == 1:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = permutation(np.array(data_[600*i:600*(i+1)]))\n",
        "        if (k+1) % 3 == 2:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = Jitter(np.array(data_[600*i:600*(i+1)]))\n",
        "    if aug_P != 0:\n",
        "        pass\n",
        "    return data_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transforms3d\n",
            "  Downloading transforms3d-0.3.1.tar.gz (62 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: transforms3d\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transforms3d: filename=transforms3d-0.3.1-py3-none-any.whl size=59373 sha256=4f684c697711596b5a312d614d42bd61d266327288a80a5b3eae98c1cbd41012\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/93/8985551f83720ce37548a5b543c75380bb707955a9c2c5d28c\n",
            "Successfully built transforms3d\n",
            "Installing collected packages: transforms3d\n",
            "Successfully installed transforms3d-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpxY2HJfynuK"
      },
      "source": [
        "데이터 변수 증가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-YXBQUwNFoI"
      },
      "source": [
        "pi = math.pi\n",
        "dt=0.02 \n",
        "from math import atan, sqrt\n",
        "\n",
        "def roll_pitch(data):\n",
        "    roll = (data.iloc[:,1]/(data.iloc[:,0]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
        "    pitch = (data.iloc[:,0]/(data.iloc[:,1]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
        "    return pd.concat([roll,pitch], axis=1)\n",
        "\n",
        "# 원본데이터를 에너지값으로 표현\n",
        "def get_energy(data):\n",
        "    energy_ = (data.iloc[:,0]**2+data.iloc[:,1]**2+data.iloc[:,2]**2)**(1/3)\n",
        "    return energy_ \n",
        "\n",
        "def get_energy_between(data,data_):\n",
        "   energy_between = ((data.iloc[:,0]-data_.iloc[:,0])**2+(data.iloc[:,1]-data_.iloc[:,1])**2+(data.iloc[:,2]-data_.iloc[:,2])**2)**(1/3)\n",
        "   return energy_between\n",
        "\n",
        "def setting(data, data_):\n",
        " \n",
        "    for i in range(0, data.shape[0], 600):\n",
        "         data[i] = data_[i] - data_[i+599]\n",
        "\n",
        "    return data\n",
        "        \n",
        "def get_diff(data):\n",
        "        x_dif, y_dif, z_dif = data.iloc[:, 0].diff(), data.iloc[:, 1].diff(), data.iloc[:, 2].diff()\n",
        "        return pd.concat([setting(x_dif, data.iloc[:, 0]),\n",
        "                      setting(y_dif, data.iloc[:, 1]),\n",
        "                      setting(z_dif, data.iloc[:, 2])], axis= 1)\n",
        "def fourier_transform_one_signal(t_signal):\n",
        "    complex_f_signal= fftpack.fft(t_signal)\n",
        "    amplitude_f_signal=np.abs(complex_f_signal)\n",
        "    return amplitude_f_signal\n",
        "\n",
        "def get_fourier(data):\n",
        "  fft=[]\n",
        "  for i in data['id'].unique():\n",
        "    temp=data.loc[data['id']==i]\n",
        "    for i in data.columns[2:8]:\n",
        "        temp[i]=fourier_transform_one_signal(temp[i].values)\n",
        "    fft.append(temp)\n",
        "  return pd.concat(fft)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5d4__REosXt"
      },
      "source": [
        "def train_dataset(data, i, aug_P = 0):\n",
        "\n",
        "    acc_data=data.iloc[:,2:5]\n",
        "    gy_data=data.iloc[:,5:8]\n",
        " \n",
        "    aug_acc = combine_aug(acc_data, i, aug_P)\n",
        "    aug_gy = combine_aug(gy_data, i, aug_P)\n",
        "\n",
        "    energy_acc = get_energy(aug_acc)\n",
        "    energy_gy  = get_energy(aug_gy)\n",
        "\n",
        "    diff_acc = get_diff(aug_acc)\n",
        "    diff_gy = get_diff(aug_gy)\n",
        "\n",
        "    roll_pitch_acc = roll_pitch(aug_acc)\n",
        "    roll_pitch_gy = roll_pitch(aug_gy)\n",
        "    \n",
        "    energy_acc_gy = get_energy_between(aug_acc, aug_gy)\n",
        "    \n",
        "    data_id_time=data.iloc[:,0:2]\n",
        "    aug_data=pd.concat([data_id_time,aug_acc,aug_gy],axis=1)\n",
        "\n",
        "    acc_gy_fourier=get_fourier(aug_data)\n",
        "\n",
        "\n",
        "    return pd.concat([acc_gy_fourier,diff_acc,energy_acc, roll_pitch_acc,diff_gy, energy_gy,  roll_pitch_gy, energy_acc_gy], axis= 1)\n",
        "\n",
        "\n",
        "def test_dataset(data):\n",
        "\n",
        "    acc_data=data.iloc[:,2:5]\n",
        "    gy_data=data.iloc[:,5:8]\n",
        "\n",
        "    roll_pitch_acc = roll_pitch(acc_data)\n",
        "    roll_pitch_gy = roll_pitch(gy_data)\n",
        "\n",
        "    energy_acc = get_energy(acc_data)\n",
        "    energy_gy  = get_energy(gy_data)\n",
        "\n",
        "    diff_acc = get_diff(acc_data)\n",
        "    diff_gy = get_diff(gy_data)\n",
        "\n",
        "    energy_acc_gy = get_energy_between(acc_data, gy_data)\n",
        "    \n",
        "    acc_gy_fourier=get_fourier(data)\n",
        "\n",
        "    return pd.concat([acc_gy_fourier,diff_acc,energy_acc,  roll_pitch_acc, diff_gy, energy_gy, roll_pitch_gy, energy_acc_gy], axis= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1mzfFrWvYKU"
      },
      "source": [
        "스케일러"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_QA4DPvX1j"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data_for_scaler = test_dataset(train).iloc[:,2:]\n",
        "scaler = StandardScaler().fit(np.array(data_for_scaler))\n",
        "\n",
        "data_for_scaler = np.array(data_for_scaler).reshape(-1, 600, data_for_scaler.shape[1])\n",
        "########################################################################################\n",
        "test_x = test_dataset(test).iloc[:,2:]\n",
        "\n",
        "test_X = scaler.transform(np.array(test_x)).reshape(-1, 600, test_x.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro3qMzNYybmb"
      },
      "source": [
        "모델링 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2jie8S3vSuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2c17b4-3404-49e5-a5ee-890841816197"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from numpy.random import seed\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1766: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvGqWlg1k5Kx"
      },
      "source": [
        "def first_model():\n",
        "    inputs = L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
        "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
        "    mp = L.MaxPool1D()(gru1)\n",
        "    ap = L.AveragePooling1D()(gru1)\n",
        "    concat1 = L.Concatenate()([mp, ap])\n",
        "    gru2 = L.GRU(256, return_sequences = True, dropout = 0.2)(concat1)\n",
        "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
        "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
        "    return keras.models.Model(inputs, dense)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFy327-7GRtZ"
      },
      "source": [
        "def cnn_model():\n",
        "    seed(2021)\n",
        "    tf.random.set_seed(2021)\n",
        "    \n",
        "    input_layer =  L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
        "    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation('relu')(conv2)\n",
        "    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\n",
        "    \n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation('relu')(conv3)\n",
        "    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n",
        "    \n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "    \n",
        "    output_layer = keras.layers.Dense(61, activation='softmax')(gap)\n",
        "    \n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5y8bvnXmHMg"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "\n",
        "def train_model(model_ = None, epochs = 40, first_rlr = 15, second_rlr = 30, r_seed = 10, aug_P = 0, seed_ = 1):\n",
        "    # first_rlr : 첫번째로 learning_rate이 감소\n",
        "    # second_rlr : 두번째로 learning_rate이 감소\n",
        "    # r_seed : StratifiedKFold seed\n",
        "    # seed_ : numpy/random seed\n",
        "    \n",
        "    result_model = []\n",
        "    cnt = 0\n",
        "    train_reshape = np.array(train).reshape(-1,600,8)\n",
        "\n",
        "    #train_reshape_acc = np.array(train_acc).reshape(-1, 600, 3)\n",
        "    #train_reshape_gy = np.array(train_gy).reshape(-1, 600, 3)\n",
        "    \n",
        "    random.seed(seed_)\n",
        "    tf.random.set_seed(21)\n",
        "\n",
        "    spliter = StratifiedKFold(n_splits=10, shuffle = True, random_state = r_seed)\n",
        "    \n",
        "    for train_split_id, valid_split_id in spliter.split(train_reshape, train_y):\n",
        "       \n",
        "        train_Y, valid_Y = np.array(pd.get_dummies(train_label.label))[train_split_id], np.array(pd.get_dummies(train_label.label))[valid_split_id]\n",
        "\n",
        "        valid_s = train_reshape[valid_split_id].reshape(-1, 8)\n",
        "        pdvalid_s = pd.DataFrame(valid_s)\n",
        "        pdvalid_s.rename(columns={0: 'id', 1:'time', 2:0, 3:1, 4:2, 5:3, 6:4, 7:5}, inplace = True)\n",
        "       \n",
        "        \n",
        "        valid_1 = test_dataset(pdvalid_s)\n",
        "        valid_x = valid_1.iloc[:,2:]\n",
        "        valid_X = scaler.transform(np.array(valid_x)).reshape(-1, 600, valid_x.shape[1])\n",
        "\n",
        "        model = model_()\n",
        "        model.compile(optimizer=keras.optimizers.RMSprop(0.003),\n",
        "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        val_score = 0\n",
        "        seed_ += 1\n",
        "\n",
        "        for i in range(epochs):\n",
        "            \n",
        "            np.random.seed(seed_*47 + i)\n",
        "            \n",
        "            train_s = train_reshape[train_split_id].reshape(-1, 8)\n",
        "            pdtrain_s = pd.DataFrame(train_s)\n",
        "            pdtrain_s.rename(columns={0: 'id', 1:'time', 2:0, 3:1, 4:2, 5:3, 6:4, 7:5}, inplace = True)\n",
        "\n",
        "            train_1 = train_dataset(pdtrain_s, i, aug_P)\n",
        "            train_x = train_1.iloc[:,2:]\n",
        "            train_X = scaler.transform(np.array(train_x)).reshape(-1, 600, train_x.shape[1])\n",
        "\n",
        "            train_X_ = train_X.copy()\n",
        "\n",
        "            train_X_ = rolling(train_X_)\n",
        "\n",
        "            hist = model.fit(train_X_, train_Y, epochs = 1, validation_data = (valid_X, valid_Y), verbose = 0)\n",
        "\n",
        "            train_accuracy = hist.history[\"accuracy\"]\n",
        "            new_val_score = accuracy_score(np.argmax(valid_Y, axis = 1), np.argmax(model.predict(valid_X), axis = 1))\n",
        "            val_loss = hist.history[\"val_loss\"]\n",
        "\n",
        "            if i == first_rlr:\n",
        "                model.compile(optimizer=keras.optimizers.RMSprop(0.003*0.2),\n",
        "                              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            if i == second_rlr:\n",
        "                model.compile(optimizer = keras.optimizers.RMSprop(0.003*0.2*0.4),\n",
        "                             loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            print(\"epoch {} - train_accuracy : {} - validation_loss : {} - validation_accuracy : {}\".format(i,\n",
        "                                                                                                            train_accuracy,\n",
        "                                                                                                            val_loss,\n",
        "                                                                                                            new_val_score,\n",
        "                                                                                                            ))\n",
        "\n",
        "            if i == 0:\n",
        "                val_loss_score = val_loss[0]\n",
        "        \n",
        "            if val_loss_score >= val_loss[0]:\n",
        "                val_loss_score = val_loss[0]\n",
        "                best_model = model\n",
        "                print(\"####best_val####\")\n",
        "                    \n",
        "            if new_val_score >= val_score:\n",
        "                val_score = new_val_score\n",
        "                best_model = model\n",
        "                print(\"####best_acc####\")\n",
        "        print(\"####################################################### cycle {} is done\".format(cnt))\n",
        "        result_model.append(best_model)\n",
        "        cnt+=1\n",
        "    return result_model\n",
        "\n",
        "\n",
        "def predict_(model):\n",
        "    result = []\n",
        "    for mod in model:\n",
        "        result.append(mod.predict(test_X))\n",
        "    predict = np.array(result).mean(axis = 0)\n",
        "    return predict\n",
        "\n",
        "def save_model(models, name = '1'):\n",
        "    cnt = 1\n",
        "    for model in models:\n",
        "        model.save(path + \"submission/last/weight/\" + name + '-{}.h5'.format(cnt))\n",
        "        cnt +=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvNiAEfBmlrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bfda182-7223-4be0-a249-0966d672ac68"
      },
      "source": [
        "first_result = train_model(cnn_model, r_seed = 2020, seed_ = 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 - train_accuracy : [0.5277382731437683] - validation_loss : [2.515937566757202] - validation_accuracy : 0.48562300319488816\n",
            "####best_val####\n",
            "####best_acc####\n",
            "epoch 1 - train_accuracy : [0.5825035572052002] - validation_loss : [1.6863915920257568] - validation_accuracy : 0.5942492012779552\n",
            "####best_val####\n",
            "####best_acc####\n",
            "epoch 2 - train_accuracy : [0.6642958521842957] - validation_loss : [1.2395613193511963] - validation_accuracy : 0.6677316293929713\n",
            "####best_val####\n",
            "####best_acc####\n",
            "epoch 3 - train_accuracy : [0.6507823467254639] - validation_loss : [1.202382206916809] - validation_accuracy : 0.6869009584664537\n",
            "####best_val####\n",
            "####best_acc####\n",
            "epoch 4 - train_accuracy : [0.7187055349349976] - validation_loss : [1.050822138786316] - validation_accuracy : 0.6996805111821086\n",
            "####best_val####\n",
            "####best_acc####\n",
            "epoch 5 - train_accuracy : [0.6945234537124634] - validation_loss : [1.0861440896987915] - validation_accuracy : 0.6996805111821086\n",
            "####best_acc####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22mWm6qd5A8p"
      },
      "source": [
        "submission.iloc[:, 1:] = predict_(first_result)\n",
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfYVxYi5R0DK"
      },
      "source": [
        "submission.to_csv('fourier_cnn'.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}