{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "광운해커톤(원본).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seunggeon/KW_AI_Hackathon/blob/main/%EA%B4%91%EC%9A%B4%ED%95%B4%EC%BB%A4%ED%86%A4_%EC%B9%BC%EB%A7%8C%ED%95%84%ED%84%B0(%EC%9B%90%EB%B3%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date Processing"
      ],
      "metadata": {
        "id": "4GociOyocfq1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8HzxlQ8JOsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc2306c-93c4-4f74-9c5f-c87ab1c74b95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie1WfnOMOsp1"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/월간 11 운동')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqZobjceOs3s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "5c29346f-b1ba-4b2c-cafd-af84f4ca9b75"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "train = pd.read_csv('train_features.csv')\n",
        "train_acc, train_gy  = train.iloc[:, 2:5], train.iloc[:, 5:] #iloc 알아두기. 데이터에 특정 행과 열을 선택.\n",
        "\n",
        "train_label = pd.read_csv('train_labels.csv')\n",
        "train_y = train_label.label\n",
        "\n",
        "test = pd.read_csv('test_features.csv')\n",
        "submission=pd.read_csv('sample_submission.csv')\n",
        "\n",
        "train_gy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gy_x</th>\n",
              "      <th>gy_y</th>\n",
              "      <th>gy_z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.591608</td>\n",
              "      <td>-30.549010</td>\n",
              "      <td>-31.676112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.303100</td>\n",
              "      <td>-39.139103</td>\n",
              "      <td>-24.927216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3.617278</td>\n",
              "      <td>-44.122565</td>\n",
              "      <td>-25.019629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.712986</td>\n",
              "      <td>-53.597843</td>\n",
              "      <td>-27.454013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.286707</td>\n",
              "      <td>-57.906561</td>\n",
              "      <td>-27.961234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874995</th>\n",
              "      <td>-29.367857</td>\n",
              "      <td>-104.013664</td>\n",
              "      <td>-76.290437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874996</th>\n",
              "      <td>-30.149089</td>\n",
              "      <td>-101.796809</td>\n",
              "      <td>-76.625087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874997</th>\n",
              "      <td>-27.873095</td>\n",
              "      <td>-98.776072</td>\n",
              "      <td>-79.365125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874998</th>\n",
              "      <td>-23.636550</td>\n",
              "      <td>-99.139495</td>\n",
              "      <td>-80.259478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874999</th>\n",
              "      <td>-17.917626</td>\n",
              "      <td>-100.181873</td>\n",
              "      <td>-80.676229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1875000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              gy_x        gy_y       gy_z\n",
              "0        -0.591608  -30.549010 -31.676112\n",
              "1         0.303100  -39.139103 -24.927216\n",
              "2        -3.617278  -44.122565 -25.019629\n",
              "3         2.712986  -53.597843 -27.454013\n",
              "4         4.286707  -57.906561 -27.961234\n",
              "...            ...         ...        ...\n",
              "1874995 -29.367857 -104.013664 -76.290437\n",
              "1874996 -30.149089 -101.796809 -76.625087\n",
              "1874997 -27.873095  -98.776072 -79.365125\n",
              "1874998 -23.636550  -99.139495 -80.259478\n",
              "1874999 -17.917626 -100.181873 -80.676229\n",
              "\n",
              "[1875000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypjxh5PK7gN5",
        "outputId": "1fa7958c-9210-4f30-9d9b-bcac48193d59"
      },
      "source": [
        "# GPU Check \n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Agument"
      ],
      "metadata": {
        "id": "_T3QUOaadZ9d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ2u-CCONs2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afa01c1-bbee-4df1-f7a2-13110e3c4ed1"
      },
      "source": [
        "import scipy\n",
        "!pip install transforms3d\n",
        "from transforms3d.axangles import axangle2mat\n",
        "\n",
        "def rolling(data):\n",
        "    for j in np.random.choice(data.shape[0], int(data.shape[0]*2/3)):\n",
        "        data[j] = np.roll(data[j], np.random.choice(data.shape[1]), axis= 0)\n",
        "    return data\n",
        "\n",
        "def rotation(data):\n",
        "    axis = np.random.uniform(low=-1, high=1, size=data.shape[1])\n",
        "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
        "    return np.matmul(data , axangle2mat(axis,angle))\n",
        "\n",
        "def permutation(data, nPerm=4, mSL=10):\n",
        "    data_new = np.zeros(data.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile == True:\n",
        "        segs = np.zeros(nPerm+1, dtype=int)\n",
        "        segs[1:-1] = np.sort(np.random.randint(mSL, data.shape[0]-mSL, nPerm-1))\n",
        "        segs[-1] = data.shape[0]\n",
        "        if np.min(segs[1:]-segs[0:-1]) > mSL:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        data_temp = data[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
        "        data_new[pp:pp+len(data_temp),:] = data_temp\n",
        "        pp += len(data_temp)\n",
        "    return(data_new)\n",
        "\n",
        "\"\"\"\n",
        "sigma = 0.05\n",
        "def Jitter(data, sigma=0.05):\n",
        "    myNoise = np.random.normal(loc=0, scale=sigma, size=data.shape)\n",
        "    return data+myNoise\n",
        "\"\"\"\n",
        "\n",
        "def combine_aug(data, k, aug_P = 0):\n",
        "    data_ = data.copy()\n",
        "    if aug_P == 0:\n",
        "        if (k+1) % 2 == 0:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = rotation(np.array(data_[600*i:600*(i+1)]))\n",
        "        if (k+1) % 2 == 1:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = permutation(np.array(data_[600*i:600*(i+1)]))\n",
        "                \n",
        "    if aug_P != 0:\n",
        "        pass\n",
        "    return data_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/1AX4XfWi6DOpBeyUiaiZuedH9Gptp9IJYzT0SWnCK6pWaxRYcyIk7qIZ-EYMCollecting transforms3d\n",
            "  Downloading transforms3d-0.3.1.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 597 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: transforms3d\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transforms3d: filename=transforms3d-0.3.1-py3-none-any.whl size=59373 sha256=c38eeee6721a55a8c7b50dcd83312f1899c4d8b62b00b19efb1d87aba0923c97\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/93/8985551f83720ce37548a5b543c75380bb707955a9c2c5d28c\n",
            "Successfully built transforms3d\n",
            "Installing collected packages: transforms3d\n",
            "Successfully installed transforms3d-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpxY2HJfynuK"
      },
      "source": [
        "## Data transform\n",
        "Eolier Angle and Energy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-YXBQUwNFoI"
      },
      "source": [
        "pi = math.pi\n",
        "dt=0.02 \n",
        "from math import atan, sqrt\n",
        "\n",
        "def roll_pitch(data):\n",
        "    roll = (data.iloc[:,1]/(data.iloc[:,0]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
        "    pitch = (data.iloc[:,0]/(data.iloc[:,1]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
        "    return pd.concat([roll,pitch], axis=1)\n",
        "\n",
        "# 원본데이터를 에너지값으로 표현\n",
        "def get_energy(data):\n",
        "    energy_ = (data.iloc[:,0]**2+data.iloc[:,1]**2+data.iloc[:,2]**2)**(1/3)\n",
        "    return energy_ \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kalman Filter"
      ],
      "metadata": {
        "id": "mbWv6YqccmOD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gALltk8dr7Js",
        "outputId": "b42d38a4-a0ab-40a0-9104-7149f37dc62e"
      },
      "source": [
        " '''\n",
        " Filename: 11_EulerKalman.py\n",
        " Created on: April,3, 2021\n",
        " Author: dhpark\n",
        " From: 칼만필터는 어렵지 않아 / https://github.com/DonghunP/Book_KalmanFilter\n",
        "'''\n",
        "\n",
        "def GetAccel(i):\n",
        "    # (41500, 1)\n",
        "    return ax, ay, az\n",
        "\n",
        "def EulerAccel(ax, ay, az):\n",
        "    theta = atan(ax/sqrt(ay**2 + az**2))\n",
        "    phi = atan(ay/az)\n",
        "    return phi, theta\n",
        "\n",
        "def EulerToQuaternion(phi, theta, psi):\n",
        "    sinPhi = sin(phi/2)\n",
        "    cosPhi = cos(phi/2)\n",
        "    sinTheta = sin(theta/2)\n",
        "    cosTheta = cos(theta/2)\n",
        "    sinPsi = sin(psi/2)\n",
        "    cosPsi = cos(psi/2)\n",
        "    z = np.array([cosPhi*cosTheta*cosPsi + sinPhi*sinTheta*sinPsi,\n",
        "                  sinPhi*cosTheta*cosPsi - cosPhi*sinTheta*sinPsi,\n",
        "                  cosPhi*sinTheta*cosPsi + sinPhi*cosTheta*sinPsi,\n",
        "                  cosPhi*cosTheta*sinPsi - sinPhi*sinTheta*cosPsi])\n",
        "    return z\n",
        "\n",
        "def EulerKalman(A, z):\n",
        "    global firstRun\n",
        "    global Q, H, R\n",
        "    global x, P\n",
        "    if firstRun:\n",
        "        H = np.eye(4)\n",
        "        Q = 0.0001 * np.eye(4)\n",
        "        R = 10 * np.eye(4)\n",
        "        x = np.array([1, 0, 0, 0]).transpose()\n",
        "        P = np.eye(4)\n",
        "        firstRun = False\n",
        "    else:\n",
        "        Xp = A @ x # Xp : State Variable Prediction\n",
        "        Pp = A @ P @ A.T + Q # Error Covariance Prediction\n",
        "\n",
        "        K = (Pp @ H.T) @ inv(H@Pp@H.T + R) # K : Kalman Gain\n",
        "\n",
        "        x = Xp + K@(z - H@Xp) # Update State Variable Estimation\n",
        "        P = Pp - K@H@Pp # Update Error Covariance Estimation\n",
        "\n",
        "    phi   = atan2(2 * (x[2] * x[3] + x[0] * x[1]), 1 - 2*(x[1]**2 + x[2]**2))\n",
        "    try:\n",
        "      theta = -asin(2 *  (x[1] * x[3] - x[0] * x[2]))\n",
        "    except ValueError:\n",
        "      theta = 0\n",
        "    psi   = atan2(2 *  (x[1] * x[2] + x[0] * x[3]), 1-2*(x[2]**2 + x[3]**2))\n",
        "    return phi, theta, psi\n",
        "\n",
        "def kalman_excute(data_acc,data_gy):\n",
        "\n",
        "  import numpy as np\n",
        "  from numpy.linalg import inv\n",
        "  import matplotlib.pyplot as plt\n",
        "  from math import cos, sin, asin, atan2, pi, atan, sqrt\n",
        "  from scipy import io\n",
        "\n",
        "  H, Q, R = None, None, None  \n",
        "  x, P = None, None\n",
        "  firstRun = True\n",
        "\n",
        "  Nsamples = 600\n",
        "  EulerSaved = np.zeros([Nsamples,3])\n",
        "  dt = 0.01\n",
        "\n",
        "  for k in range(Nsamples):\n",
        "    p = data_gy.iloc[k,0]  \n",
        "    q = data_gy.iloc[k,1]  \n",
        "    r = data_gy.iloc[k,2] \n",
        "\n",
        "    A = np.eye(4) + dt * (1/2) * np.array([[0,-p,-q,-r],[p,0,r,-q],[q,-r,0,p],[r,q,-p,0]])\n",
        "    ax = data_acc.iloc[k,0]   # (41500, 1)\n",
        "    ay = data_acc.iloc[k,1]  # (41500, 1)\n",
        "    az = data_acc.iloc[k,2] \n",
        "    phi, theta = EulerAccel(ax, ay, az)\n",
        "    z = EulerToQuaternion(phi, theta, 0) #State variable as Quaternion form\n",
        "\n",
        "    phi, theta, psi = EulerKalman(A, z)\n",
        "    EulerSaved[k] = [phi, theta, psi]\n",
        "\n",
        "  t = np.arange(0, Nsamples * dt ,dt)\n",
        "  PhiSaved = EulerSaved[:,0] * 180/pi\n",
        "  ThetaSaved = EulerSaved[:,1] * 180/pi\n",
        "  PsiSaved = EulerSaved[:,2] * 180/pi\n",
        "\n",
        "  PhiSaved = pd.DataFrame(PhiSaved)\n",
        "  ThetaSaved = pd.DataFrame(ThetaSaved)\n",
        "  \n",
        "  return pd.concat([PhiSaved,ThetaSaved], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nplt.figure()\\nplt.plot(t, PhiSaved)\\nplt.xlabel('Time [Sec]')\\nplt.ylabel('Roll angle [deg]')\\nplt.savefig('11_EulerKalman_roll.png')\\n\\nplt.figure()\\nplt.plot(t, ThetaSaved)\\nplt.xlabel('Time [Sec]')\\nplt.ylabel('Pitch angle [deg]')\\nplt.savefig('11_EulerKalman_pitch.png')\\nplt.show()\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train_dataset & Test Dataset"
      ],
      "metadata": {
        "id": "00YbpyaBeoX2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdjBPhZ6Cu5b"
      },
      "source": [
        "def train_dataset(acc_data, gy_data, i, aug_P = 0):\n",
        "\n",
        "    aug_acc = combine_aug(acc_data, i, aug_P)\n",
        "    aug_gy = combine_aug(gy_data, i, aug_P)\n",
        "    \n",
        "    energy_acc = get_energy(aug_acc)\n",
        "    energy_gy  = get_energy(aug_gy)\n",
        "\n",
        "    kalam_data = kalman_excute(acc_data, gy_data)\n",
        "\n",
        "    return pd.concat([aug_acc, kalam_data, energy_acc, energy_gy ,\n",
        "                     aug_gy], axis= 1)\n",
        "\n",
        "def test_dataset(acc_data, gy_data):\n",
        "    \n",
        "    energy_acc = get_energy(acc_data)\n",
        "    energy_gy  = get_energy(gy_data)\n",
        "\n",
        "    kalam_data = kalman_excute(acc_data, gy_data)\n",
        "\n",
        "    return pd.concat([acc_data, kalam_data, energy_acc, energy_gy,\n",
        "                      gy_data], axis= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWTDnmSvDQQ0"
      },
      "source": [
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data_for_scaler = train_dataset(train_acc, train_gy,1) # train data만 사용\n",
        "scaler = StandardScaler().fit(np.array(data_for_scaler))\n",
        "\n",
        "data_for_scaler = np.array(data_for_scaler).reshape(-1, 600, data_for_scaler.shape[1])\n",
        "########################################################################################\n",
        "test_x = test_dataset(test.iloc[:, 2:5], test.iloc[:, 5:])\n",
        "\n",
        "test_X = scaler.transform(np.array(test_x)).reshape(-1, 600, test_x.shape[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "9fQGksoUco7g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2jie8S3vSuz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK3rm_apN8VD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b3c5449a-8fb8-4748-95ff-3a30caf0ccfa"
      },
      "source": [
        "'''\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM,Bidirectional,Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K \n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from numpy.random import seed\n",
        "import keras\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n!pip install tensorflow-addons\\nimport tensorflow_addons as tfa\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, LSTM,Bidirectional,Dropout\\nfrom keras.layers import Dense\\nfrom keras.layers import Dropout\\nfrom keras.layers import LSTM\\nfrom keras.layers.convolutional import Conv1D\\nfrom keras.layers.convolutional import MaxPooling1D\\nfrom keras.utils import to_categorical\\nfrom keras import backend as K \\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\\nfrom sklearn.model_selection import KFold,StratifiedKFold\\nfrom numpy.random import seed\\nimport keras\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU Model"
      ],
      "metadata": {
        "id": "RbRzyvLJe0WC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvGqWlg1k5Kx"
      },
      "source": [
        "def first_model():\n",
        "    inputs = L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
        "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
        "    mp = L.MaxPool1D()(gru1)\n",
        "    ap = L.AveragePooling1D()(gru1)\n",
        "    concat1 = L.Concatenate()([mp, ap])\n",
        "    gru2 = L.GRU(256, return_sequences = True, dropout = 0.2)(concat1)\n",
        "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
        "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
        "    return keras.models.Model(inputs, dense)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN model"
      ],
      "metadata": {
        "id": "IgqPeFPme37o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5gsO5-N9Ef"
      },
      "source": [
        "def cnn_model(input_shape, classes):\n",
        "    seed(2021)\n",
        "    tf.random.set_seed(2021)\n",
        "    \n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
        "    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation('relu')(conv2)\n",
        "    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\n",
        "    \n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation('relu')(conv3)\n",
        "    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n",
        "    \n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "    \n",
        "    output_layer = keras.layers.Dense(classes, activation='softmax')(gap)\n",
        "    \n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "TgYDZd4Ge8Sb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5y8bvnXmHMg"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "import sklearn\n",
        "\n",
        "def train_model(model_ = None, epochs = 40, first_rlr = 15, second_rlr = 30, r_seed = 10, aug_P = 0, seed_ = 1):\n",
        "    # first_rlr : 첫번째로 learning_rate이 감소\n",
        "    # second_rlr : 두번째로 learning_rate이 감소\n",
        "    # r_seed : StratifiedKFold seed\n",
        "    # seed_ : numpy/random seed\n",
        "    \n",
        "    result_model = []\n",
        "    cnt = 0\n",
        "    train_reshape_acc = np.array(train_acc).reshape(-1, 600, 3)\n",
        "    train_reshape_gy = np.array(train_gy).reshape(-1, 600, 3)\n",
        "    \n",
        "    random.seed(seed_)\n",
        "    tf.random.set_seed(21)\n",
        "\n",
        "    spliter = StratifiedKFold(n_splits=10, shuffle = True, random_state = r_seed)\n",
        "    for train_split_id, valid_split_id in spliter.split(train_reshape_acc, train_y):\n",
        "        \n",
        "        train_Y, valid_Y = np.array(pd.get_dummies(train_label.label))[train_split_id], np.array(pd.get_dummies(train_label.label))[valid_split_id]\n",
        "\n",
        "        valid_ACC, valid_GY = train_reshape_acc[valid_split_id].reshape(-1, 3), train_reshape_gy[valid_split_id].reshape(-1, 3)\n",
        "        valid_x = test_dataset(pd.DataFrame(valid_ACC), pd.DataFrame(valid_GY))\n",
        "        valid_X = scaler.transform(np.array(valid_x)).reshape(-1, 600, valid_x.shape[1])\n",
        "\n",
        "        model = model_()\n",
        "        model.compile(optimizer=keras.optimizers.RMSprop(0.003),\n",
        "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        val_score = 0\n",
        "        seed_ += 1\n",
        "\n",
        "        for i in range(epochs):\n",
        "            \n",
        "            np.random.seed(seed_*47 + i)\n",
        "            \n",
        "            train_ACC, train_GY = train_reshape_acc[train_split_id].reshape(-1, 3), train_reshape_gy[train_split_id].reshape(-1, 3)\n",
        "            train_x = train_dataset(pd.DataFrame(train_ACC), pd.DataFrame(train_GY), i, aug_P)\n",
        "            train_X = scaler.transform(np.array(train_x)).reshape(-1, 600, valid_x.shape[1])\n",
        "\n",
        "            train_X_ = train_X.copy()\n",
        "\n",
        "            train_X_ = rolling(train_X_)\n",
        "\n",
        "            hist = model.fit(train_X_, train_Y, epochs = 1, validation_data = (valid_X, valid_Y), verbose = 0)\n",
        "\n",
        "            train_accuracy = hist.history[\"accuracy\"]\n",
        "            new_val_score = accuracy_score(np.argmax(valid_Y, axis = 1), np.argmax(model.predict(valid_X), axis = 1))\n",
        "            val_loss = hist.history[\"val_loss\"]\n",
        "\n",
        "            if i == first_rlr:\n",
        "                model.compile(optimizer=keras.optimizers.RMSprop(0.003*0.2),\n",
        "                              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            if i == second_rlr:\n",
        "                model.compile(optimizer = keras.optimizers.RMSprop(0.003*0.2*0.4),\n",
        "                             loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            print(\"epoch {} - train_accuracy : {} - validation_loss : {} - validation_accuracy : {}\".format(i,\n",
        "                                                                                                            train_accuracy,\n",
        "                                                                                                            val_loss,\n",
        "                                                                                                            new_val_score,\n",
        "                                                                                                            ))\n",
        "\n",
        "            if i == 0:\n",
        "                val_loss_score = val_loss[0]\n",
        "        \n",
        "            if val_loss_score >= val_loss[0]:\n",
        "                val_loss_score = val_loss[0]\n",
        "                best_model = model\n",
        "                print(\"####best_val####\")\n",
        "                    \n",
        "            if new_val_score >= val_score:\n",
        "                val_score = new_val_score\n",
        "                best_model = model\n",
        "                print(\"####best_acc####\")\n",
        "        print(\"####################################################### cycle {} is done\".format(cnt))\n",
        "        result_model.append(best_model)\n",
        "        cnt+=1\n",
        "\n",
        "        \n",
        "    return result_model\n",
        "\n",
        "\n",
        "def predict_(model):\n",
        "    result = []\n",
        "    for mod in model:\n",
        "        result.append(mod.predict(test_X))\n",
        "    predict = np.array(result).mean(axis = 0)\n",
        "\n",
        "    return predict\n",
        "\n",
        "def save_model(models, name = '1'):\n",
        "    cnt = 1\n",
        "    for model in models:\n",
        "        model.save(path + \"submission/last/weight/\" + name + '-{}.h5'.format(cnt))\n",
        "        cnt +=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "n46dDXo_fDeM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvNiAEfBmlrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b7fbcc-2e22-4eed-e7c7-b922e0d2cedd"
      },
      "source": [
        "first_result = train_model(first_model, r_seed = 47, seed_ = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 1 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 2 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 3 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 4 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 5 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 6 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 7 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 8 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 9 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 10 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 11 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 12 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 13 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 14 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 15 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 16 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 17 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 18 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 19 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 20 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 21 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 22 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 23 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 24 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 25 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 26 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 27 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 28 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 29 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 30 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 31 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 32 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n",
            "epoch 33 - train_accuracy : [0.003911806736141443] - validation_loss : [nan] - validation_accuracy : 0.003194888178913738\n",
            "####best_acc####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Result"
      ],
      "metadata": {
        "id": "9FQmv13OfG_j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfYVxYi5R0DK"
      },
      "source": [
        "submission.to_csv('baseline_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}