{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "광운해커톤(칼만).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8HzxlQ8JOsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6227fc3-7e59-4b72-acef-ab72765ef40c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie1WfnOMOsp1"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/월간 11 운동')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-DB5DCXsTaO",
        "outputId": "60d4e6e4-3e40-43a7-9579-f00d5ba635d0"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct  6 15:11:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPC__UQ8sQak",
        "outputId": "8b47d71f-fbc0-4645-d7bb-4145960bdd57"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqZobjceOs3s"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "train = pd.read_csv('train_features.csv')\n",
        "train_acc, train_gy  = train.iloc[:, 2:5], train.iloc[:, 5:] #iloc 알아두기. 데이터에 특정 행과 열을 선택.\n",
        "\n",
        "train_label = pd.read_csv('train_labels.csv')\n",
        "train_y = train_label.label\n",
        "\n",
        "test = pd.read_csv('test_features.csv')\n",
        "submission=pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UGPMrsxWGLc"
      },
      "source": [
        "# Nsamples =600\n",
        "# EulerSaved = np.zeros([Nsamples,3])\n",
        "# EulerSaved.shape"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOaeYp9f-7-d"
      },
      "source": [
        "**GPU 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypjxh5PK7gN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a303b9f-0858-43d6-b7e2-6998c18ba863"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoCUgT0T_Dto"
      },
      "source": [
        "# EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT12nOND_Ot5"
      },
      "source": [
        "# DATA Augment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ2u-CCONs2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0ca314-9ddb-4ea3-eec9-60ab2b2edd05"
      },
      "source": [
        "import scipy\n",
        "!pip install transforms3d\n",
        "from transforms3d.axangles import axangle2mat\n",
        "\n",
        "def rolling(data):\n",
        "    for j in np.random.choice(data.shape[0], int(data.shape[0]*2/3)):\n",
        "        data[j] = np.roll(data[j], np.random.choice(data.shape[1]), axis= 0)\n",
        "    return data\n",
        "\n",
        "def rotation(data):\n",
        "    axis = np.random.uniform(low=-1, high=1, size=data.shape[1])\n",
        "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
        "    return np.matmul(data , axangle2mat(axis,angle))\n",
        "\n",
        "def permutation(data, nPerm=4, mSL=10):\n",
        "    data_new = np.zeros(data.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile == True:\n",
        "        segs = np.zeros(nPerm+1, dtype=int)\n",
        "        segs[1:-1] = np.sort(np.random.randint(mSL, data.shape[0]-mSL, nPerm-1))\n",
        "        segs[-1] = data.shape[0]\n",
        "        if np.min(segs[1:]-segs[0:-1]) > mSL:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        data_temp = data[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
        "        data_new[pp:pp+len(data_temp),:] = data_temp\n",
        "        pp += len(data_temp)\n",
        "    return(data_new)\n",
        "\n",
        "\"\"\"\n",
        "sigma = 0.05\n",
        "def Jitter(data, sigma=0.05):\n",
        "    myNoise = np.random.normal(loc=0, scale=sigma, size=data.shape)\n",
        "    return data+myNoise\n",
        "\"\"\"\n",
        "\n",
        "def combine_aug(data, k, aug_P = 0):\n",
        "    data_ = data.copy()\n",
        "    if aug_P == 0:\n",
        "        if (k+1) % 2 == 0:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = rotation(np.array(data_[600*i:600*(i+1)]))\n",
        "        if (k+1) % 2 == 1:\n",
        "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
        "                data_[600*i:600*(i+1)] = permutation(np.array(data_[600*i:600*(i+1)]))\n",
        "                \n",
        "    if aug_P != 0:\n",
        "        pass\n",
        "    return data_\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transforms3d\n",
            "  Downloading transforms3d-0.3.1.tar.gz (62 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 1.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: transforms3d\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transforms3d: filename=transforms3d-0.3.1-py3-none-any.whl size=59373 sha256=551f2c7b213db31c98dc39facdea775e8b0eca4152b946c50c2706fec337e069\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/93/8985551f83720ce37548a5b543c75380bb707955a9c2c5d28c\n",
            "Successfully built transforms3d\n",
            "Installing collected packages: transforms3d\n",
            "Successfully installed transforms3d-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpxY2HJfynuK"
      },
      "source": [
        "# 주어진 dataset를 모델에 넣을 데이터로 변환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sY2XWfzAZeV"
      },
      "source": [
        "#에너지 변환 및 Roll & Pitch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-YXBQUwNFoI"
      },
      "source": [
        "# pi = math.pi\n",
        "# dt=0.02 \n",
        "# from math import atan, sqrt\n",
        "\n",
        "# def roll_pitch(data):\n",
        "#     roll = (data.iloc[:,1]/(data.iloc[:,0]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
        "#     pitch = (data.iloc[:,0]/(data.iloc[:,1]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
        "#     return pd.concat([roll,pitch], axis=1)\n",
        "\n",
        "# # 원본데이터를 에너지값으로 표현\n",
        "# def get_energy(data):\n",
        "#     energy_ = (data.iloc[:,0]**2+data.iloc[:,1]**2+data.iloc[:,2]**2)**(1/3)\n",
        "#     return energy_ "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKMSQT5sr-mg"
      },
      "source": [
        "# 칼만필터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLRIHH3w91OI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4838451-b00c-4de1-b813-0b912857c397"
      },
      "source": [
        "    !pip install -q tensorflow-gpu==2.0.0-rc1\n",
        "    !pip install transformations\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "    from numpy.linalg import inv\n",
        "    import matplotlib.pyplot as plt\n",
        "    from math import cos, sin, asin, atan2, pi, atan, sqrt\n",
        "    from scipy import io\n",
        "    from transformations import euler_from_quaternion\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 380.5 MB 9.1 kB/s \n",
            "\u001b[K     |████████████████████████████████| 501 kB 59.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 36.7 MB/s \n",
            "\u001b[?25hCollecting transformations\n",
            "  Downloading transformations-2021.6.6.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from transformations) (1.19.5)\n",
            "Building wheels for collected packages: transformations\n",
            "  Building wheel for transformations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformations: filename=transformations-2021.6.6-cp37-cp37m-linux_x86_64.whl size=113507 sha256=82c5a3ca7018f5dd53a7dc6fcb66e7de94a3f7732bffe8ec637c04b4c4db099c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/3c/16/a67f60f017d0003f27b7156162df6f3aa45e4b2cee82232e3a\n",
            "Successfully built transformations\n",
            "Installing collected packages: transformations\n",
            "Successfully installed transformations-2021.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gALltk8dr7Js",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bb9dc464-6b1a-4ad3-b59e-d66176f6c3b5"
      },
      "source": [
        " '''\n",
        " Filename: 11_EulerKalman.py\n",
        " Created on: April,3, 2021\n",
        " Author: dhpark\n",
        "'''\n",
        "def kalman_excute(train_acc,train_gy):\n",
        "\n",
        "    def GetGyro(i):\n",
        "        p = train_gy.iloc[i,0]  # (41500, 1)\n",
        "        q = train_gy.iloc[i,1]  # (41500, 1)\n",
        "        r = train_gy.iloc[i,2]  # (41500, 1)\n",
        "        \n",
        "        return p, q, r\n",
        "\n",
        "    def GetAccel(i):\n",
        "        ax = train_acc.iloc[i,0]   # (41500, 1)\n",
        "        ay = train_acc.iloc[i,1]  # (41500, 1)\n",
        "        az = train_acc.iloc[i,2] \n",
        "        \n",
        "        return ax, ay, az\n",
        "        \n",
        "    def EulerAccel(ax, ay, az):\n",
        "        theta = atan(ax/sqrt(ay**2 + az**2))\n",
        "        phi = atan(ay/az)\n",
        "        return phi, theta\n",
        "\n",
        "    def EulerToQuaternion(phi, theta, psi):\n",
        "        sinPhi = sin(phi/2)\n",
        "        cosPhi = cos(phi/2)\n",
        "        sinTheta = sin(theta/2)\n",
        "        cosTheta = cos(theta/2)\n",
        "        sinPsi = sin(psi/2)\n",
        "        cosPsi = cos(psi/2)\n",
        "        z = np.array([cosPhi*cosTheta*cosPsi + sinPhi*sinTheta*sinPsi,\n",
        "                      sinPhi*cosTheta*cosPsi - cosPhi*sinTheta*sinPsi,\n",
        "                      cosPhi*sinTheta*cosPsi + sinPhi*cosTheta*sinPsi,\n",
        "                      cosPhi*cosTheta*sinPsi - sinPhi*sinTheta*cosPsi])\n",
        "        return z\n",
        "\n",
        "    def EulerKalman(A, z, k, Q, H, R, x, P):\n",
        "        \n",
        "        Xp = A @ x # Xp : State Variable Prediction\n",
        "        Pp = A @ P @ A.T + Q # Error Covariance Prediction\n",
        "\n",
        "        K = (Pp @ H.T) @ inv(H@Pp@H.T + R) # K : Kalman Gain\n",
        "\n",
        "        x = Xp + K@(z - H@Xp) # Update State Variable Estimation\n",
        "        P = Pp - K@H@Pp # Update Error Covariance Estimation\n",
        "       \n",
        "\n",
        "        phi, theta, psi = euler_from_quaternion(x)\n",
        "            \n",
        "        # phi   = atan2(2 * (x[2] * x[3] + x[0] * x[1]), 1 - 2*(x[1]**2 + x[2]**2))\n",
        "        # try:\n",
        "        #     theta = -asin(2 *  (x[1] * x[3] - x[0] * x[2]))\n",
        "        # except:\n",
        "        #     theta = -asin()\n",
        "        # psi   = atan2(2 *  (x[1] * x[2] + x[0] * x[3]), 1-2*(x[2]**2 + x[3]**2))\n",
        "        return phi, theta, psi\n",
        "\n",
        "    Nsamples = train_acc.shape[0]\n",
        "    EulerSaved = np.zeros([Nsamples,3])\n",
        "    dt = 0.01\n",
        "\n",
        "    H, Q, R = None, None, None  \n",
        "    x, P = None, None\n",
        "\n",
        "    for k in range(Nsamples):\n",
        "        p, q, r = GetGyro(k)\n",
        "        A = np.eye(4) + dt * (1/2) * np.array([[0,-p,-q,-r],[p,0,r,-q],[q,-r,0,p],[r,q,-p,0]])\n",
        "        ax, ay, az = GetAccel(k)\n",
        "        phi, theta = EulerAccel(ax, ay, az)\n",
        "        z = EulerToQuaternion(phi, theta, 0) #State variable as Quaternion form\n",
        "        \n",
        "        H = np.eye(4)\n",
        "        Q = 0.0001 * np.eye(4)\n",
        "        R = 10 * np.eye(4)\n",
        "        x = np.array([1, 0, 0, 0]).transpose()\n",
        "        P = np.eye(4)\n",
        "        phi, theta, psi = EulerKalman(A, z, k, Q, H, R, x, P)\n",
        "        EulerSaved[k] = [phi, theta, psi]\n",
        "       \n",
        "\n",
        "    PhiSaved = EulerSaved[:,0] * 180/pi\n",
        "    ThetaSaved = EulerSaved[:,1] * 180/pi\n",
        "    PsiSaved = EulerSaved[:,2] * 180/pi\n",
        "\n",
        "\n",
        "    PhiSaved = pd.DataFrame(PhiSaved)\n",
        "    ThetaSaved = pd.DataFrame(ThetaSaved)\n",
        "    \n",
        "    return pd.concat([PhiSaved,ThetaSaved], axis=1)\n",
        "\n",
        "\n",
        "'''\n",
        "plt.figure()\n",
        "plt.plot(t, PhiSaved)\n",
        "plt.xlabel('Time [Sec]')\n",
        "plt.ylabel('Roll angle [deg]')\n",
        "plt.savefig('11_EulerKalman_roll.png')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t, ThetaSaved)\n",
        "plt.xlabel('Time [Sec]')\n",
        "plt.ylabel('Pitch angle [deg]')\n",
        "plt.savefig('11_EulerKalman_pitch.png')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nplt.figure()\\nplt.plot(t, PhiSaved)\\nplt.xlabel('Time [Sec]')\\nplt.ylabel('Roll angle [deg]')\\nplt.savefig('11_EulerKalman_roll.png')\\n\\nplt.figure()\\nplt.plot(t, ThetaSaved)\\nplt.xlabel('Time [Sec]')\\nplt.ylabel('Pitch angle [deg]')\\nplt.savefig('11_EulerKalman_pitch.png')\\nplt.show()\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue1bm761AHvv"
      },
      "source": [
        "# 변환한 데이터를 DataFrame화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdjBPhZ6Cu5b"
      },
      "source": [
        "def train_dataset(acc_data, gy_data):\n",
        "\n",
        "    kalam_data = kalman_excute(acc_data, gy_data)\n",
        "\n",
        "    return pd.concat([acc_data, gy_data, kalam_data], axis= 1)\n",
        "\n",
        "def test_dataset(acc_data, gy_data):\n",
        "    \n",
        "    kalam_data = kalman_excute(acc_data, gy_data)\n",
        "\n",
        "    return pd.concat([acc_data, gy_data, kalam_data], axis= 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8hhJkBSA2_a"
      },
      "source": [
        "# Scale Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_z_VyCH2yku"
      },
      "source": [
        "#Scaler fit 및 Dataset 리턴값 확인하기."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCIlcvkIXOJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "bdc292c5-5105-4e4d-b6a2-f54079264d76"
      },
      "source": [
        "     '''\n",
        "      import sklearn\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      scaler = StandardScaler()\n",
        "\n",
        "      train_reshape_acc = np.array(train_acc).reshape(-1, 600, 3)\n",
        "      train_reshape_gy = np.array(train_gy).reshape(-1, 600, 3)\n",
        "\n",
        "      for i in range(1000):\n",
        "        train_ACC, train_GY = train_reshape_acc[i].reshape(-1, 3), train_reshape_gy[i].reshape(-1, 3)\n",
        "        data_for_scaler = train_dataset(pd.DataFrame(train_ACC), pd.DataFrame(train_GY))\n",
        "        print(data_for_scaler)\n",
        "        scaler = StandardScaler().fit(data_for_scaler) #스탠다드 스케일러 사용\n",
        "\n",
        "        scale_X = np.array(data_for_scaler).reshape(-1, 600, data_for_scaler.shape[1])\n",
        "      #print(scale_X)\n",
        "      '''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n import sklearn\\n from sklearn.preprocessing import StandardScaler\\n scaler = StandardScaler()\\n\\n train_reshape_acc = np.array(train_acc).reshape(-1, 600, 3)\\n train_reshape_gy = np.array(train_gy).reshape(-1, 600, 3)\\n\\n for i in range(1000):\\n   train_ACC, train_GY = train_reshape_acc[i].reshape(-1, 3), train_reshape_gy[i].reshape(-1, 3)\\n   data_for_scaler = train_dataset(pd.DataFrame(train_ACC), pd.DataFrame(train_GY))\\n   print(data_for_scaler)\\n   scaler = StandardScaler().fit(data_for_scaler) #스탠다드 스케일러 사용\\n\\n   scale_X = np.array(data_for_scaler).reshape(-1, 600, data_for_scaler.shape[1])\\n #print(scale_X)\\n '"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urBehSuOA7A3"
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# Xtrain_scaled = pd.DataFrame(data=scaler.fit_transform(Xtrain[total_feature_names]), index=Xtrain.index, columns=total_feature_names)\n",
        "# Xtest_scaled = pd.DataFrame(data=scaler.transform(Xtest[total_feature_names]), index=Xtest.index, columns=total_feature_names)\n",
        "\n",
        "# print(Xtrain_scaled.shape, Xtest_scaled.shape)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGw1EyiFBx1d"
      },
      "source": [
        "Reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdklYjm9B1Mj"
      },
      "source": [
        "# Xtrain_scaled = np.array(Xtrain_scaled[total_feature_names]).reshape(-1, 600, len(total_feature_names)).astype('float32')\n",
        "# Xtest_scaled= np.array(Xtest_scaled[total_feature_names]).reshape(-1, 600, len(total_feature_names)).astype('float32')\n",
        "\n",
        "# Ytrain = Ytrain['label']\n",
        "\n",
        "# print(Xtrain_scaled.shape, Ytrain.shape, Xtest_scaled.shape, submission.shape)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro3qMzNYybmb"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2jie8S3vSuz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK3rm_apN8VD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d87d7e76-61bd-439e-98e6-05f40d506a47"
      },
      "source": [
        "'''\n",
        "#1D - CNN\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM,Bidirectional,Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K \n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from numpy.random import seed\n",
        "import keras\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#1D - CNN\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n!pip install tensorflow-addons\\nimport tensorflow_addons as tfa\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, LSTM,Bidirectional,Dropout\\nfrom keras.layers import Dense\\nfrom keras.layers import Dropout\\nfrom keras.layers import LSTM\\nfrom keras.layers.convolutional import Conv1D\\nfrom keras.layers.convolutional import MaxPooling1D\\nfrom keras.utils import to_categorical\\nfrom keras import backend as K \\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\\nfrom sklearn.model_selection import KFold,StratifiedKFold\\nfrom numpy.random import seed\\nimport keras\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HENVaI0mAutI"
      },
      "source": [
        "# Model definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5gsO5-N9Ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "1195bb30-d0f7-4569-f405-023d2aaadb10"
      },
      "source": [
        "'''\n",
        "def cnn_model(input_shape, classes):\n",
        "    seed(2021)\n",
        "    tf.random.set_seed(2021)\n",
        "    \n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
        "    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation('relu')(conv2)\n",
        "    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\n",
        "    \n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation('relu')(conv3)\n",
        "    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n",
        "    \n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "    \n",
        "    output_layer = keras.layers.Dense(classes, activation='softmax')(gap)\n",
        "    \n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "'''"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef cnn_model(input_shape, classes):\\n    seed(2021)\\n    tf.random.set_seed(2021)\\n    \\n    input_layer = keras.layers.Input(input_shape)\\n    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\\n    conv1 = keras.layers.BatchNormalization()(conv1)\\n    conv1 = keras.layers.Activation(activation='relu')(conv1)\\n    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\\n\\n    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\\n    conv2 = keras.layers.BatchNormalization()(conv2)\\n    conv2 = keras.layers.Activation('relu')(conv2)\\n    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\\n    \\n    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\\n    conv3 = keras.layers.BatchNormalization()(conv3)\\n    conv3 = keras.layers.Activation('relu')(conv3)\\n    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\\n    \\n    gap = keras.layers.GlobalAveragePooling1D()(conv3)\\n    \\n    output_layer = keras.layers.Dense(classes, activation='softmax')(gap)\\n    \\n    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\\n    \\n    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \\n        metrics=['accuracy'])\\n    \\n    return model\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32qVOP9WYL2p"
      },
      "source": [
        "def first_model(input) :\n",
        "\n",
        "    \n",
        "    inputs = L.Input(shape = (input.shape[1], input.shape[2]))\n",
        "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
        "    mp = L.MaxPool1D()(gru1)\n",
        "    ap = L.AveragePooling1D()(gru1)\n",
        "    concat1 = L.Concatenate()([mp, ap])\n",
        "    gru2 = L.GRU(256, return_sequences = True, dropout = 0.2)(concat1)\n",
        "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
        "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
        "    return keras.models.Model(inputs, dense)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2DhI0vGL7OJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5f3fc8a-7d10-40ba-d968-3d30c5e2444c"
      },
      "source": [
        "\"\"\"\n",
        "train_split_id = [1,2,5,7]\n",
        "train_reshape_acc[train_split_id].shape\n",
        "\"\"\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntrain_split_id = [1,2,5,7]\\ntrain_reshape_acc[train_split_id].shape\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXm_QxfeRdn-"
      },
      "source": [
        "# i = list(range(10))\n",
        "# print(i)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv9FougNJXuH"
      },
      "source": [
        "# b = [1,2,3]\n",
        "# my_2darray = np.array([[1, 2, 3], [4, 5, 6],[6,74,7],[7,4,3],[7,86,3]])\n",
        "# a= pd.DataFrame(my_2darray)\n",
        "# print(c)\n",
        "# c=np.array(a)\n",
        "# print(c[b])\n",
        "# x = pd.DataFrame(np.array(a)[b])\n",
        "# print(x)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr3zc9C8XLQh"
      },
      "source": [
        "    # train_reshape_acc = np.array(train_acc).reshape(-1, 600, 3)\n",
        "    # train_reshape_gy = np.array(train_gy).reshape(-1, 600, 3)\n",
        "    \n",
        "    # #print(train_reshape_acc, \"train reshape \\n\")\n",
        "    \n",
        "\n",
        "    # #i = list(range(0, 3125))\n",
        "    # i = [1,2,3]\n",
        "    # k=[1,2]\n",
        "    # train_ACC, train_GY = train_reshape_acc[i].reshape(-1, 3), train_reshape_gy[i].reshape(-1, 3)\n",
        "    # x =[]\n",
        "   \n",
        "    # print(train_ACC)\n",
        "    # for id in k :\n",
        "    #   a = 600*(id-1)\n",
        "    #   b = 600*id\n",
        "    #   x.append(train_ACC[a:b])\n",
        "    # y = np.array(x)\n",
        "    # x1 = y.reshape(y.shape[0]*y.shape[1],y.shape[2])  \n",
        "    # pd.DataFrame(x1)\n",
        "    # print(\"x1: \\n\",x1\n",
        "    #       )\n",
        "    # print(\"x1: \\n\",x1.shape)\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwm-asWGBQdR"
      },
      "source": [
        "# Train Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5y8bvnXmHMg",
        "outputId": "5b260808-9db0-45d7-97ac-72df8e2184ca"
      },
      "source": [
        "    from sklearn.model_selection import StratifiedKFold\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    import random\n",
        "    import sklearn\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "     \n",
        "\n",
        "    # first_rlr : 첫번째로 learning_rate이 감소\n",
        "    # second_rlr : 두번째로 learning_rate이 감소\n",
        "    # r_seed : StratifiedKFold seed\n",
        "    #eed_ : numpy/random seed\n",
        "    \n",
        "    epochs = 40\n",
        "    first_rlr = 15\n",
        "    second_rlr = 30\n",
        "    r_seed = 2020\n",
        "    seed_ = 25\n",
        "\n",
        "    train_reshape_acc = np.array(train_acc).reshape(-1, 600, 3)\n",
        "    train_reshape_gy = np.array(train_gy).reshape(-1, 600, 3)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    i = list(range(0, 3125))\n",
        "    #i = list(range(0, 100))\n",
        "    train_ACC, train_GY = train_reshape_acc[i].reshape(-1, 3), train_reshape_gy[i].reshape(-1, 3)\n",
        "    data_for_scaler = train_dataset(pd.DataFrame(train_ACC), pd.DataFrame(train_GY))\n",
        "    scale_X = scaler.fit_transform(data_for_scaler) #스탠다드 스케일러 사용\n",
        "\n",
        "    scale_X = np.array(scale_X).reshape(-1, 600, data_for_scaler.shape[1])\n",
        "    # train_scaler_ACC, train_scaler_GY = train_reshape_acc.reshape(-1, 3), train_reshape_gy.reshape(-1, 3)\n",
        "    # data_for_scaler = train_dataset(pd.DataFrame(train_scaler_ACC), pd.DataFrame(train_scaler_GY))\n",
        "    \n",
        "    # scaler = StandardScaler().fit(np.array(data_for_scaler)) #스탠다드 스케일러 사용\n",
        "    # data_for_scaler = np.array(data_for_scaler).reshape(-1, 600, data_for_scaler.shape[1])\n",
        "\n",
        "    result_model = []\n",
        "    cnt = 0\n",
        "    \n",
        "    random.seed(seed_)\n",
        "    tf.random.set_seed(21)\n",
        "\n",
        "    print(scale_X.shape[1],\"Scale\" ,scale_X.shape[2])\n",
        "    model = first_model(scale_X)\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(0.003),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    spliter = StratifiedKFold(n_splits=10, shuffle = True, random_state = r_seed)\n",
        "\n",
        "    for train_split_id, valid_split_id in spliter.split(train_reshape_acc, train_y):\n",
        "        \n",
        "        train_Y, valid_Y = np.array(pd.get_dummies(train_y))[train_split_id], np.array(pd.get_dummies(train_y))[valid_split_id]\n",
        "        valid_ACC, valid_GY = train_reshape_acc[valid_split_id].reshape(-1, 3), train_reshape_gy[valid_split_id].reshape(-1, 3)\n",
        "        valid_x = test_dataset(pd.DataFrame(valid_ACC), pd.DataFrame(valid_GY))\n",
        "        print(\"valid_x\",\"\\n\",valid_x,\"\\n\")\n",
        "        valid_X = scaler.transform(np.array(valid_x)).reshape(-1, 600, valid_x.shape[1])\n",
        " \n",
        "       \n",
        "        val_score = 0\n",
        "        seed_ += 1\n",
        "\n",
        "        for i in range(epochs):\n",
        "            \n",
        "            np.random.seed(seed_*47 + i)\n",
        "\n",
        "            # print(\"train_split_id\",train_split_id,\"\\n\")\n",
        "            # print(\"valid_split_id\",valid_split_id,\"\\n\")\n",
        "            # train_ACC, train_GY = train_reshape_acc[train_split_id].reshape(-1, 3), train_reshape_gy[train_split_id].reshape(-1, 3)\n",
        "\n",
        "            # # print(\"train_ACC\",train_ACC,\"\\n\")\n",
        "            # # print(\"train_GY\",train_GY,\"\\n\")\n",
        "            # train_x = train_dataset(pd.DataFrame(train_ACC), pd.DataFrame(train_GY))\n",
        "            # print(\"train_x\",\"\\n\",train_x,\"\\n\")\n",
        " \n",
        "            train_x = []\n",
        "            # print(train_split_id)\n",
        "            # print(data_for_scaler)\n",
        "            \n",
        "            a, b = train_id_slice(train_split_id, data_for_scaler)    \n",
        "            train_x.extend(np.array(data_for_scaler)[a:b])\n",
        "            \n",
        "            train_x_arr = np.array(train_x)\n",
        "            # pd.DataFrame(train_x_arr)\n",
        "            # print(\"train_x_arr\",\"\\n\",train_x_arr,\"\\n\")\n",
        "            # print(\"train_x_arr\",\"\\n\",train_x_arr.shape,\"\\n\")\n",
        "            # train_x_arr1 = train_x_arr.reshape(-1,3) \n",
        "            # print(\"train_x_arr1\",\"\\n\",train_x_arr1,\"\\n\")\n",
        "            train_X = scaler.transform(train_x_arr).reshape(-1, 600, pd.DataFrame(train_x_arr).shape[1])\n",
        "        \n",
        "            train_X_ = train_X.copy()\n",
        "\n",
        "            train_X_ = rolling(train_X_)\n",
        "            # print(\"train_X_\",train_X_,\"\\n\", \"train_X:\" ,train_X,\"\\n\")\n",
        "            hist = model.fit(train_X_, train_Y, epochs = 1, validation_data = (valid_X, valid_Y), verbose = 0)\n",
        "\n",
        "            train_accuracy = hist.history[\"accuracy\"]\n",
        "            new_val_score = accuracy_score(np.argmax(valid_Y, axis = 1), np.argmax(model.predict(valid_X), axis = 1))\n",
        "            val_loss = hist.history[\"val_loss\"]\n",
        "\n",
        "            if i == first_rlr:\n",
        "                model.compile(optimizer=keras.optimizers.RMSprop(0.003*0.2),\n",
        "                              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            if i == second_rlr:\n",
        "                model.compile(optimizer = keras.optimizers.RMSprop(0.003*0.2*0.4),\n",
        "                             loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            print(\"epoch {} - train_accuracy : {} - validation_loss : {} - validation_accuracy : {}\".format(i,\n",
        "                                                                                                            train_accuracy,\n",
        "                                                                                                            val_loss,\n",
        "                                                                                                            new_val_score,\n",
        "                                                                                                            ))\n",
        "\n",
        "            if i == 0:\n",
        "                val_loss_score = val_loss[0]\n",
        "        \n",
        "            if val_loss_score >= val_loss[0]:\n",
        "                val_loss_score = val_loss[0]\n",
        "                best_model = model\n",
        "                print(\"####best_val####\")\n",
        "                    \n",
        "            if new_val_score >= val_score:\n",
        "                val_score = new_val_score\n",
        "                best_model = model\n",
        "                print(\"####best_acc####\")\n",
        "        print(\"####################################################### cycle {} is done\".format(cnt))\n",
        "        result_model.append(best_model)\n",
        "        cnt+=1\n",
        "\n",
        "    def train_id_slice(train_split_id, data_for_scaler):\n",
        "              for id in train_split_id:\n",
        "              \n",
        "                a = 600*id\n",
        "                b = 600*(id+1)\n",
        "\n",
        "                yield a, b\n",
        "\n",
        "    def predict_(model):\n",
        "        result = []\n",
        "        for mod in model:\n",
        "            result.append(mod.predict(test_X))\n",
        "        predict = np.array(result).mean(axis = 0)\n",
        "\n",
        "        return predict\n",
        "\n",
        "    def save_model(models, name = '1'):\n",
        "        cnt = 1\n",
        "        for model in models:\n",
        "            model.save(path + \"submission/last/weight/\" + name + '-{}.h5'.format(cnt))\n",
        "            cnt +=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojDaNp_lFtel"
      },
      "source": [
        "# model = first_model(scale_X)\n",
        "# print(model.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqrkxAkeBaqN"
      },
      "source": [
        "#시작"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1cyvly8-4Y2"
      },
      "source": [
        "제출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfYVxYi5R0DK"
      },
      "source": [
        "submission.to_csv('kalman_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}